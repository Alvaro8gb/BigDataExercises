{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8669e76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97072e7c-f604-4e34-a8bf-e249da6de9b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Spark Context with SparkConf\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\").setAppName(\"app\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae534bca-0f72-4863-8401-338818efd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFilePath=\"pagecounts\"\n",
    "rdd = sc.textFile(inputFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aea2eb3-1676-406d-9dfc-c09fb1360ba4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o23.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/mikel/Desktop/Master/BigData/BigDataPracticalWork/app/pagecounts\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Exercise 1\u001b[39;00m\n\u001b[0;32m      3\u001b[0m rdd1 \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrdd1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line)\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\pyspark\\rdd.py:1850\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1826\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m \u001b[39mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1847\u001b[0m \u001b[39m[91, 92, 93]\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m items: List[T] \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1850\u001b[0m totalParts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgetNumPartitions()\n\u001b[0;32m   1851\u001b[0m partsScanned \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   1853\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(items) \u001b[39m<\u001b[39m num \u001b[39mand\u001b[39;00m partsScanned \u001b[39m<\u001b[39m totalParts:\n\u001b[0;32m   1854\u001b[0m     \u001b[39m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[0;32m   1855\u001b[0m     \u001b[39m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[0;32m   1856\u001b[0m     \u001b[39m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\pyspark\\rdd.py:3491\u001b[0m, in \u001b[0;36mPipelinedRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3490\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetNumPartitions\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m-> 3491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prev_jrdd\u001b[39m.\u001b[39;49mpartitions()\u001b[39m.\u001b[39msize()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\spark\\spark-3.3.4-bin-hadoop2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o23.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/c:/Users/mikel/Desktop/Master/BigData/BigDataPracticalWork/app/pagecounts\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "\n",
    "rdd1 = rdd.map(lambda line: line.split(' '))\n",
    "for line in rdd1.take(10):\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c177878-0375-4a50-b86a-0e9a2529eebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "\n",
    "numLines = rdd.count()\n",
    "print('Number of pages:',numLines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c213d-89c3-4f0b-90f4-4e0725a31a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "\n",
    "# using the mapped rdd\n",
    "enPages1 = rdd1.filter(lambda line: line[0].__eq__('en')) # this is the same as line[0] == 'en'\n",
    "# using the raw rdd\n",
    "enPages2 = rdd.filter(lambda line: line.startswith('en')) # :)\n",
    "enPages = enPages2\n",
    "\n",
    "numEnLines1 = enPages1.count()\n",
    "numEnLines2 = enPages2.count()\n",
    "print('\\nNumber of EN pages:',numEnLines1)\n",
    "print('\\nNumber of EN pages:',numEnLines2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdf50e-fb57-4237-a0bb-d3df9a219e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "\n",
    "enPagesTuples = enPages.flatMap(lambda line: [(pieces[0], pieces[1], int(pieces[2]), int(pieces[3]))\n",
    "                                                 for pieces in [line.split(\" \")] if len(pieces) == 4])\n",
    "for line in enPagesTuples.take(10):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6aa18-75dd-481a-a180-eff060ac13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5\n",
    "\n",
    "topSortedEnPages = enPagesTuples.sortBy(lambda x: x[2], ascending=False) \\\n",
    ".take(5) \\\n",
    "\n",
    "for page in topSortedEnPages:\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea2883-7ee7-4517-b303-432c044a50f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "top = enPagesTuples.sortBy(lambda x: x[2], ascending=False).first()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('Name: ' + top[1] + \"\\tNumber of Visists: \" + str(top[2]))\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be37db9f-1dd8-4d25-8d10-86ad26aa951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "# Another option. Lower complexity\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "maxValue = enPagesTuples.map(lambda x: x[2]).reduce(lambda x, y: max(x, y))\n",
    "top = enPagesTuples.filter(lambda x: x[2] == maxValue).first()\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('Name: ' + top[1] + \"\\tNumber of Visists: \" + str(top[2]))\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9760a-eb16-4f4b-88b3-bcc714c00408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "# Yet another, even better option\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "top = enPagesTuples.reduce(lambda t1, t2: t1 if t1[2] > t2[2] else t2)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print('Name: ' + top[1] + \"\\tNumber of Visists: \" + str(top[2]))\n",
    "print(f\"Execution time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af1fea0-0ff4-40a3-8a31-68526bc9d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7\n",
    "\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "def histogram(page_rdd: RDD, n_bins: int) -> RDD:\n",
    "    # First, calculate the bounds (min and max)\n",
    "    bounds = page_rdd.map(lambda x: (x[2], x[2])).reduce(lambda t1, t2: (min(t1[0], t2[0]), max(t1[1], t2[1])))\n",
    "\n",
    "    hist_range = bounds[1] - bounds[0]\n",
    "    bin_width = hist_range / n_bins\n",
    "    print('bounds:' ,bounds)\n",
    "    for i in range(0,n_bins-1):\n",
    "        print(bounds[0]+bin_width*i,bounds[0]+ bin_width*(i+1))\n",
    "\n",
    "    histogram_result = page_rdd.map(lambda t: ((t[2] - bounds[0]) // bin_width) * bin_width + bounds[0]) \\\n",
    "            .groupBy(lambda x: x) \\\n",
    "            .map(lambda t: (t[0], len(list(t[1])))) \\\n",
    "            .sortBy(lambda x: x[0])\n",
    "    \n",
    "\n",
    "    return histogram_result\n",
    "    \n",
    "print('\\n Histogram bins:')\n",
    "for item in histogram(enPagesTuples, 20).collect():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de05e645-3d74-4f86-9e1e-6f94c16dfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7 Pro\n",
    "\n",
    "from pyspark import RDD\n",
    "\n",
    "def create_histogram(page_rdd: RDD, n_bins: int):\n",
    "    # First, calculate the bounds (min and max)\n",
    "    bounds = page_rdd.map(lambda x: (x[2], x[2])) \\\n",
    "                    .reduce(lambda t1, t2: (min(t1[0], t2[0]), max(t1[1], t2[1])))\n",
    "\n",
    "    hist_range = bounds[1] - bounds[0]\n",
    "    bin_width = hist_range / n_bins\n",
    "\n",
    "    histogram_result = page_rdd.map(lambda t: ((t[2] - bounds[0]) // bin_width) * bin_width + bounds[0]) \\\n",
    "                           .groupBy(lambda x: x) \\\n",
    "                           .map(lambda t: (t[0], len(list(t[1])))) \\\n",
    "                           .sortBy(lambda x: x[0]) \\\n",
    "                           .collect()  # Collect the results to print\n",
    "\n",
    "    max_count = max(histogram_result, key=lambda x: x[1])[1]\n",
    "    print('Histogram:')\n",
    "    for bin_start, count in histogram_result:\n",
    "        bin_end = bin_start + bin_width\n",
    "        bar_length = int(40 * count / max_count)  # Adjust the scale for visualization\n",
    "        print(f\"{bin_start:.2f} - {bin_end:.2f}: {'*' * bar_length} ({count})\")\n",
    "\n",
    "# Assuming you have enPagesTuples and nBins defined elsewhere\n",
    "create_histogram(enPagesTuples, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98506e-cf88-4a5d-a4a7-be45fb11ed5b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfdb90f-9315-410d-89f8-be98f157a6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "690f2f857305324ec80385bbcd0cbcceb9ff9c570252c9b3adda08028153e17a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
