{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.ml.feature as feature\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FlightAnalysisApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5270893\n"
     ]
    }
   ],
   "source": [
    "#DATA_2008 = \"../ds/flight-2008.csv.bz2\"\n",
    "DATA_1990 = \"../ds/flight-1990.csv.bz2\"\n",
    "#flights_df1 = spark.read.csv(DATA_2008, header=True, inferSchema=True)\n",
    "flights_df2 = spark.read.csv(DATA_1990, header=True, inferSchema=True)\n",
    "#print(flights_df1.count())\n",
    "print(flights_df2.count())\n",
    "TARGET = \"ArrDelay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flights_df = flights_df1.union(flights_df2)\n",
    "flights_df = flights_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [\n",
    "    \"ArrTime\", \"ActualElapsedTime\", \"AirTime\", \"TaxiIn\",\n",
    "    \"Diverted\", \"CarrierDelay\", \"WeatherDelay\", \"NASDelay\",\n",
    "    \"SecurityDelay\", \"LateAircraftDelay\"\n",
    "]\n",
    "\n",
    "flights_df = flights_df.drop(*features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_result = (\n",
    "    flights_df\n",
    "    .groupBy(\"Year\", \"Month\")\n",
    "    .agg({\"ArrDelay\": \"avg\", \"DepDelay\": \"avg\", \"Distance\": \"sum\"})\n",
    "    .orderBy(\"Year\", \"Month\")\n",
    ")\n",
    "analysis_result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.printSchema()\n",
    "# flights_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Calculate mean value\n",
    " ###THIS CODE IS FOR REPLACING THE NULL VALUES WITH THE MEAN VALUE OF THE COLUMN\n",
    "# mean_val = flights_df.select(mean(flights_df[TARGET])).collect()[0][0]\n",
    "# flights_df = flights_df.na.fill(mean_val, subset=[TARGET])\n",
    "flights_df = flights_df.withColumn('ArrDelay', flights_df['ArrDelay'].cast(IntegerType()))\n",
    "flights_df = flights_df.withColumn('DepDelay', flights_df['DepDelay'].cast(IntegerType()))\n",
    "\n",
    "# Calculate the mean value of the 'Distance' column\n",
    "mean_distance = flights_df.select(mean(col('Distance'))).collect()[0][0]\n",
    "\n",
    "# Replace all non-numeric values in the 'Distance' column with the mean value\n",
    "flights_df = flights_df.withColumn('Distance', when(col('Distance').cast('integer').isNull(), mean_distance).otherwise(col('Distance').cast('integer')))\n",
    "\n",
    "flights_df.printSchema()\n",
    "#flights_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check for Missing Values**: If a column has a high number of missing values, it might not be very useful for prediction. You can check the number of missing values in a column with `df.filter(df[\"column\"].isNull()).count()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values in Year: 0 (0.00%)\n",
      "Number of missing values in Month: 0 (0.00%)\n",
      "Number of missing values in DayofMonth: 0 (0.00%)\n",
      "Number of missing values in DayOfWeek: 0 (0.00%)\n",
      "Number of missing values in DepTime: 0 (0.00%)\n",
      "Number of missing values in CRSDepTime: 0 (0.00%)\n",
      "Number of missing values in CRSArrTime: 0 (0.00%)\n",
      "Number of missing values in UniqueCarrier: 0 (0.00%)\n",
      "Number of missing values in FlightNum: 0 (0.00%)\n",
      "Number of missing values in TailNum: 0 (0.00%)\n",
      "Number of missing values in CRSElapsedTime: 0 (0.00%)\n",
      "Number of missing values in ArrDelay: 68412 (1.30%)\n",
      "Warning: ArrDelay has at least one missing value , consider dropping those rows or replacing them.\n",
      "Number of missing values in DepDelay: 52458 (1.00%)\n",
      "Warning: DepDelay has at least one missing value , consider dropping those rows or replacing them.\n",
      "Number of missing values in Origin: 0 (0.00%)\n",
      "Number of missing values in Dest: 0 (0.00%)\n",
      "Number of missing values in Distance: 0 (0.00%)\n",
      "Number of missing values in TaxiOut: 0 (0.00%)\n",
      "Number of missing values in Cancelled: 0 (0.00%)\n",
      "Number of missing values in CancellationCode: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "selected_features = []\n",
    "total_rows = flights_df.count()\n",
    "for column in flights_df.columns:\n",
    "    missing_count = flights_df.filter(flights_df[column].isNull()).count()\n",
    "    missing_percentage = (missing_count / total_rows) * 100\n",
    "    print(f\"Number of missing values in {column}: {missing_count} ({missing_percentage:.2f}%)\")\n",
    "    if missing_percentage > 50:\n",
    "        print(f\"Dropping {column} due to high percentage of missing values.\")\n",
    "        flights_df = flights_df.drop(column)\n",
    "    elif missing_percentage > 30:\n",
    "        print(f\"Warning: {column} has more than a 30% of missing values, consider dropping it.\")\n",
    "        selected_features.append(column)\n",
    "    elif missing_count>0:\n",
    "        print(f\"Warning: {column} has at least one missing value , consider dropping those rows or replacing them.\")\n",
    "        selected_features.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features.remove('CancellationCode')\n",
    "flights_df = flights_df.drop('CancellationCode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering ArrDelay, type: int\n",
      "Number of rows before filtering: 5270893\n",
      "Number of rows after filtering: 5202481\n",
      "Filtering DepDelay, type: int\n",
      "Number of rows before filtering: 5202481\n",
      "Number of rows after filtering: 5202481\n"
     ]
    }
   ],
   "source": [
    "# filtered_df = flights_df.limit(100000)\n",
    "dtype_dict = dict(flights_df.dtypes)\n",
    "for i in selected_features:\n",
    "    print(f\"Filtering {i}, type: {dtype_dict[i]}\")\n",
    "    # if dtype_dict[i] == IntegerType():\n",
    "    #     print(f\"Number of rows before filtering: {flights_df.count()}\")\n",
    "    #     flights_df = flights_df.filter(col(i).isNotNull() & col(i).cast(\"integer\").isNotNull())\n",
    "    #     print(f\"Number of rows after filtering: {flights_df.count()}\")\n",
    "    print(f\"Number of rows before filtering: {flights_df.count()}\")\n",
    "    flights_df = flights_df.filter(col(i).isNotNull())\n",
    "    print(f\"Number of rows after filtering: {flights_df.count()}\")\n",
    "# flights_df = flights_df.filter(col(TARGET).isNotNull() & col(TARGET).cast(\"integer\").isNotNull())\n",
    "# print(f\"Number of rows after filtering: {flights_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "To decide which variables to drop, you can perform exploratory data analysis (EDA). Here are some steps you can follow:\n",
    "\n",
    "1. **Check for Unique Values**: If a column has a unique value for every row (like an ID), it won't be useful for prediction. You can check the number of distinct values in a column with `df.select(\"column\").distinct().count()`. If the count is equal to the number of rows in your DataFrame, you can drop the column.\n",
    "2. **Check for Constant Values**: If a column has the same value for every row, it also won't be useful for prediction. You can check this with `df.select(\"column\").distinct().count()`. If the count is 1, you can drop the column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_rows = flights_df.count()\n",
    "print(f\"Number of rows in the dataframe: {num_of_rows}\")\n",
    "for featur in flights_df.columns:\n",
    "    c1 = flights_df.select(featur).distinct().count() \n",
    "    print(f\"Feature {featur} has {c1} distinct values\")\n",
    "    if c1 == num_of_rows or c1 == 1:\n",
    "        flights_df = flights_df.drop(featur)\n",
    "        print(f\"Feature dropped: {featur}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()\n",
    "#flights_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#flights_df = flights_df.withColumn('Distance', flights_df['Distance'].cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Check for High Correlation**: If two columns are highly correlated, they carry similar information, and you can drop one of them. You can calculate the correlation between two columns with `df.stat.corr(\"column1\", \"column2\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'CRSDepTime',\n",
       " 'CRSArrTime',\n",
       " 'FlightNum',\n",
       " 'CRSElapsedTime',\n",
       " 'ArrDelay',\n",
       " 'DepDelay',\n",
       " 'Distance',\n",
       " 'Cancelled']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_vars = ['Month', 'DayofMonth', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'CRSArrTime']\n",
    "#create an array of the numerical variables except for time variabes\n",
    "numerical_vars= [item[0] for item in flights_df.dtypes if item[1].startswith('int') | item[1].startswith('double')][1:]\n",
    "#vars = list(set(numerical_vars) - set(time_vars))\n",
    "vars = numerical_vars\n",
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "#vars=numerical_vars\n",
    "for i in range(len(vars)):\n",
    "    for j in range(i+1, len(vars)):\n",
    "        correlation = flights_df.stat.corr(vars[i], vars[j])\n",
    "        if correlation > 0.75 or correlation < -0.75:\n",
    "            print(f\"ATTENTION: Correlation between {vars[i]} and {vars[j]} is too high: {correlation}, consider dropping one of them.\")\n",
    "        else:\n",
    "            print(f\"Correlation between {vars[i]} and {vars[j]}: {correlation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = feature.VectorAssembler(inputCols=vars,outputCol=\"features\")\n",
    "data = assembler.transform(flights_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3639915\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test sets\n",
    "(train_data, test_data) = data.randomSplit([0.7, 0.3])\n",
    "print(\"Training Dataset Count: \" + str(train_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Linear Regression Model\n",
    "# lr = LinearRegression(featuresCol=\"features\",labelCol=TARGET)\n",
    "# lr_model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RegParam: 0.01\n",
      "Best ElasticNetParam: 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Assuming you have 'train_data' as your training dataset and 'TARGET' as the target column\n",
    "# Define the Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=TARGET)\n",
    "\n",
    "# Create a pipeline with the Linear Regression model\n",
    "pipeline = Pipeline(stages=[lr])\n",
    "\n",
    "# Set up the parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator for regression tasks\n",
    "evaluator = RegressionEvaluator(labelCol=TARGET, metricName=\"rmse\")\n",
    "\n",
    "# Create a CrossValidator with the Linear Regression model, parameter grid, and evaluator\n",
    "cross_validator = CrossValidator(estimator=pipeline,\n",
    "                                 estimatorParamMaps=param_grid,\n",
    "                                 evaluator=evaluator,\n",
    "                                 numFolds=5)  # You can adjust the number of folds as needed\n",
    "\n",
    "# Run cross-validation to find the best model\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "\n",
    "# Get the best model from the cross-validation\n",
    "best_lr_model = cv_model.bestModel.stages[0]\n",
    "\n",
    "# Optionally, you can print the best hyperparameters\n",
    "print(\"Best RegParam:\", best_lr_model.getRegParam())\n",
    "print(\"Best ElasticNetParam:\", best_lr_model.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9996001704832248,0.0,0.0,0.0]\n",
      "Intercept: 0.002723639988620084\n",
      "numIterations: 23\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|0.026064085219204003|\n",
      "|0.010070904548186377|\n",
      "|0.002074314212682893|\n",
      "|0.008871415997859344|\n",
      "|4.749961455816631...|\n",
      "|0.006872268413985694|\n",
      "|0.001274655179132722|\n",
      "|-7.24492404743593E-4|\n",
      "|-0.00472278757249...|\n",
      "|-0.00552244660604...|\n",
      "|-0.00432295805572...|\n",
      "|-3.24662887968507...|\n",
      "|-0.00472278757249...|\n",
      "|-0.00552244660604...|\n",
      "|4.749961455816631...|\n",
      "|-7.24492404743593E-4|\n",
      "|0.002873973246233...|\n",
      "|-0.00272363998862...|\n",
      "|-0.01112005984089...|\n",
      "|0.003273802763008149|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE: 0.010000178809415262\n",
      "r2: 0.9999998401363576\n"
     ]
    }
   ],
   "source": [
    "# Print model coefficients and intercept\n",
    "print(f\"Coefficients: {best_lr_model.coefficients}\")\n",
    "print(f\"Intercept: {best_lr_model.intercept}\")\n",
    "\n",
    "# Get training summary\n",
    "trainingSummary = best_lr_model.summary\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"numIterations: {trainingSummary.totalIterations}\")\n",
    "trainingSummary.residuals.show()\n",
    "print(f\"RMSE: {trainingSummary.rootMeanSquaredError}\")\n",
    "print(f\"r2: {trainingSummary.r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.00999665\n",
      "Mean Absolute Error (MAE) on test data = 0.00573577\n",
      "R-squared on test data = 1\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "predictions = best_lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"ArrDelay\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# Root Mean Squared Error (RMSE)\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = evaluator.evaluate(predictions, {evaluator.metricName: \"mae\"})\n",
    "print(\"Mean Absolute Error (MAE) on test data = %g\" % mae)\n",
    "\n",
    "# R-squared\n",
    "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
    "print(\"R-squared on test data = %g\" % r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9367758b914eda5e9656a2e8cb31fa6c1a33028a9235e34dbffc853c5c7deb38"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
